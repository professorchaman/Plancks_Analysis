{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:1000000px;  /* your desired max-height here */\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "# This cell allows for all the output to be displayed without scrolling (up to the max-height var in pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpers is already installed\n",
      "ipywidgets is already installed\n",
      "IPython is already installed\n",
      "scipy is already installed\n",
      "numpy is already installed\n",
      "matplotlib is already installed\n",
      "tqdm is already installed\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "# List of required libraries\n",
    "libraries = [\n",
    "    'helpers',\n",
    "    'ipywidgets',\n",
    "    'IPython',\n",
    "    'scipy',\n",
    "    'numpy',\n",
    "    'matplotlib',\n",
    "    'tqdm'\n",
    "]\n",
    "\n",
    "# Check if each library is installed, and install if necessary\n",
    "for library in libraries:\n",
    "    try:\n",
    "        importlib.import_module(library)\n",
    "        print(f\"{library} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"{library} is not installed. Installing...\")\n",
    "        !pip install {library}\n",
    "        print(f\"{library} installed successfully\")\n",
    "\n",
    "# Import the required packages\n",
    "from helpers import *\n",
    "from ipywidgets import *\n",
    "from IPython.display import display, Javascript\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import medfilt\n",
    "from scipy import signal\n",
    "import peakutils\n",
    "from peakutils.plot import plot as pplot\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from scipy import stats\n",
    "from numpy.fft import fft, fftfreq, ifft\n",
    "import glob\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 6.626e-34 #Planck's constant in Js\n",
    "heV = 4.136e-15 #Planck's constant in eVs\n",
    "c = 2.9979e8 #Speed of light in m/s\n",
    "kb = 1.381e-23 #Boltzmann constant in J/K\n",
    "kbeV = 8.617e-5 #Boltzmann constant in eV/K\n",
    "scale = 2e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python4esac.github.io/fitting/example_blackbody.html\n",
    "\n",
    "def blackbody_lam(lam, T):\n",
    "    \"\"\" Blackbody as a function of wavelength (um) and temperature (K).\n",
    "\n",
    "    returns units of W/m^2-nm\n",
    "    \"\"\"\n",
    "    #from scipy.constants import h,k,c\n",
    "    lam = 10**-9*lam # convert to metres\n",
    "    \n",
    "    fun_val = 3.14*(2*h*c**2/(lam**5*(np.exp(h*c/(lam*kb*T))-1)))/10**9\n",
    "    \n",
    "    return fun_val/np.max(fun_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackbody_lam_2(lam, T):\n",
    "    \"\"\" Blackbody as a function of wavelength (um) and temperature (K).\n",
    "\n",
    "    returns units of W/m^2-nm\n",
    "    \"\"\"\n",
    "    #from scipy.constants import h,k,c\n",
    "    lam = 10**-9*lam # convert to metres\n",
    "    \n",
    "    return 3.14*(2*h*c**2/(lam**5*(np.exp(h*c/(lam*kb*T))-1)))/10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_2(f):\n",
    "    \n",
    "    data = pd.read_csv(f, sep=None, engine='python', names=['col_a_data','col_b_data'], skiprows=14)\n",
    "    x = data['col_a_data'].to_numpy()\n",
    "    y = data['col_b_data'].to_numpy()\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_3(f):\n",
    "    \n",
    "    data = pd.read_csv(f, sep=None, engine='python', names=['col_a_data','col_b_data'])\n",
    "    x = data['col_a_data'].to_numpy()\n",
    "    y = data['col_b_data'].to_numpy()\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(f,i):\n",
    "    \n",
    "    data = pd.read_csv(f[i], sep=None, engine='python', names=['col_a_data','col_b_data'])\n",
    "    x = data['col_a_data'].to_numpy()\n",
    "    y = data['col_b_data'].to_numpy()\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data,erp):\n",
    "    \n",
    "    # Code to remove cosmic rays\n",
    "    #erp=100 # Set percentage error limit\n",
    "    err=erp/100\n",
    "\n",
    "    Ndatapts = len(data)\n",
    "    ydata = data\n",
    "\n",
    "    for i in range(0,Ndatapts): # This loop removes hot regions one pixel wide\n",
    "        if i>0 and i<Ndatapts-1:\n",
    "            if ydata[i]>(1+err)*ydata[i+1] and ydata[i]>(1+err)*ydata[i-1]:\n",
    "                #print('Entered the loop 1')\n",
    "                ydata[i]=(ydata[i-1]+ydata[i+1])/2\n",
    "\n",
    "    for i in range(0,Ndatapts): # This loop removes hot regions up to 3 pixels wide\n",
    "        if i>1 and i<Ndatapts-2 and ydata[i]>(1+err)*ydata[i+2] and ydata[i]>(1+err)*ydata[i-2]:\n",
    "            #print('Entered the loop 3')\n",
    "            ydata[i], ydata[i-1], ydata[i+1] = (ydata[i-2]+ydata[i+2])/2, (ydata[i]+ydata[i-2])/2, (ydata[i+2]+ydata[i])/2\n",
    "\n",
    "    for i in range(0,Ndatapts): # This loop removes hot regions up to 5 pixels wide\n",
    "        if i>4 and i<Ndatapts-5 and ydata[i]>(1+err)*ydata[i+5] and ydata[i]>(1+err)*ydata[i-5]:\n",
    "            #print('Entered the loop 3')\n",
    "            ydata[i], ydata[i-1], ydata[i-2], ydata[i-3], ydata[i-4], ydata[i+1], ydata[i+2], ydata[i+3], ydata[i+4] = (ydata[i-5]+ydata[i+5])/2, (ydata[i]+ydata[i-2])/2, (ydata[i-1]+ydata[i-3])/2, (ydata[i-2]+ydata[i-4])/2, (ydata[i-3]+ydata[i-5])/2, (ydata[i+2]+ydata[i])/2, (ydata[i+3]+ydata[i+1])/2, (ydata[i+4]+ydata[i+2])/2, (ydata[i+5]+ydata[i+3])/2\n",
    "\n",
    "    for i in range(0,Ndatapts): # This loop removes dead regions one pixel wide\n",
    "        if i>0 and i<Ndatapts-1 and ydata[i]<(1-err)*ydata[i+1] and ydata[i]<(1-err)*ydata[i-1]:\n",
    "            #print('Entered the loop 4')\n",
    "            ydata[i]=(ydata[i-1]+ydata[i+1])/2\n",
    "\n",
    "    for i in range(0,Ndatapts): # This loop removes dead regions up to 3 pixels wide\n",
    "        if i>1 and i<Ndatapts-2 and ydata[i]<(1-err)*ydata[i+2] and ydata[i]<(1-err)*ydata[i-2]:\n",
    "            #print('Entered the loop 5')\n",
    "            ydata[i], ydata[i-1], ydata[i+1] =(ydata[i-2]+ydata[i+2])/2, (ydata[i]+ydata[i-2])/2, (ydata[i+2]+ydata[i])/2\n",
    "\n",
    "    #plt.plot(wvl,ydata)\n",
    "    #plt.show()\n",
    "    \n",
    "    return ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_bsl(data,p_order):\n",
    "    \n",
    "    base = peakutils.baseline(data,p_order)\n",
    "    \n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_savgol(data,k_size,p_order):\n",
    "    \n",
    "    data[np.isnan(data)] = 0\n",
    "    \n",
    "    sav_filt_data=signal.savgol_filter(data,k_size,p_order)\n",
    "    \n",
    "    return sav_filt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_median(data,k_size):\n",
    "    \n",
    "    filt_data = medfilt(data,kernel_size=k_size)\n",
    " \n",
    "    #plt.figure()\n",
    "    #plt.plot(x_data,filt_data,label='Median Filtered Data')\n",
    "    #plt.plot(wvl,y_data/np.max(y_data),label='Actual Data')\n",
    "    #plt.plot(peak_x,peak_y/np.max(intens_data),'ob')\n",
    "    #plt.title(fdata[i])\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    return filt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i_corr(f,flamp):\n",
    "    start = 0\n",
    "    end = -1\n",
    "\n",
    "    head_i, tail_i = os.path.split(f)\n",
    "    \n",
    "    ## Important note to change the location of this .txt file below to where it is in your computer.\n",
    "    calibstd = np.loadtxt(r\"G:\\Shared drives\\Pauzauskie Team Drive\\Users\\CG\\Scripts\\030410638_HL-2000-CAL_2014-01-15-14-09_VISEXT1EXT2_FIB.txt\")\n",
    "    xcalib = calibstd[:,0]\n",
    "    ycalib = calibstd[:,1]\n",
    "    \n",
    "    x,y = get_data_2(f)\n",
    "\n",
    "    # if len(fdark)!=0: #Does background correction if chosen\n",
    "    #     xdark, ydark = masterRead(fdark,0)\n",
    "    #     xdark=np.asarray(xdark, dtype=np.float32)\n",
    "    #     ydark=np.asarray(ydark, dtype=np.float32)\n",
    "    #     y=y-ydark\n",
    "    HglampFunc = CubicSpline(xcalib,ycalib)\n",
    "    hglampI = HglampFunc(x) # Create interpolation of true lamp spectrum\n",
    "\n",
    "    hglampdata_x, hglampdata_y = get_data(flamp,0) # Split true lamp spectra into x and y\n",
    "\n",
    "    ICF = hglampI/(hglampdata_y) # Creates ratio of true lamp spectra to real lamp data, ICF = Intensity Correction Factor\n",
    "    #print(ICF)\n",
    "    ynew = (y)*ICF # multiplies real data by intensity correction factor\n",
    "    #print(ynew)\n",
    "    \n",
    "    ynew = np.nan_to_num(ynew,nan=0,posinf=0,neginf=0)\n",
    "    datamatrix = np.column_stack((x,ynew)) # Compiles corrected data into a new matrix\n",
    "    savename = tail_i[:-4]+\"_calib.txt\" # Create filename for new data\n",
    "    #print(savename)\n",
    "    np.savetxt(os.path.join(head_i,\"Calibrated_files\",savename),datamatrix) # Save new data\n",
    "    \n",
    "    #plt.figure()\n",
    "    #plt.plot(x,ynew/np.max(ynew),label='Intensity Corrected Data')\n",
    "    #plt.plot(x,y/np.max(y),label='Actual Data')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "\n",
    "    \n",
    "    return x,y,ynew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_averaging(selectFiles,average,batching, batch_size, final_file_name='-averaged_data.csv'):\n",
    "    \n",
    "    fdata = selectFiles.files\n",
    "    \n",
    "    if len(fdata) < 2:\n",
    "        print(\"Please select at least 2 files to average\")\n",
    "        \n",
    "    head_i, tail_i = os.path.split(fdata[0])\n",
    "    y_all = []\n",
    "    for idx, file in enumerate(fdata):\n",
    "    \n",
    "        x_data, y_data, metadata = DataReader(file_name=file).read_file()\n",
    "        y_all.append(y_data)\n",
    "    \n",
    "    column_vectors = [np.expand_dims(arr, axis=1) for arr in y_all]\n",
    "\n",
    "    # Concatenate the column vectors along the axis=1\n",
    "    concatenated_array = np.concatenate(column_vectors, axis=1)\n",
    "    average_array = concatenated_array\n",
    "\n",
    "    average_batches = []\n",
    "\n",
    "    if batching==True:\n",
    "        # Get the total number of elements and batches\n",
    "        # print(concatenated_array.shape)\n",
    "        total_elements = concatenated_array.shape[1]\n",
    "        total_batches = total_elements // batch_size\n",
    "\n",
    "        # Calculate the median within each batch\n",
    "        for i in range(total_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            batch_concat = np.concatenate(column_vectors[:][start_idx:end_idx], axis=1)\n",
    "            batch_average = np.average(batch_concat, axis=1)\n",
    "            average_batches.append(batch_average)\n",
    "\n",
    "            # Save each batch_median to a CSV file\n",
    "            batch_filename = f'-batch_average_{i+1}.csv'\n",
    "            total_median_data = np.column_stack((x_data, batch_average))\n",
    "            np.savetxt(os.path.join(head_i,tail_i[:-13]+batch_filename), total_median_data, delimiter=',')\n",
    "\n",
    "    \n",
    "    averaged_data = np.average(y_all, axis=1)\n",
    "    total_average_data = np.column_stack((x_data, averaged_data))\n",
    "    np.savetxt(os.path.join(head_i,tail_i[:-13]+final_file_name), total_average_data, delimiter=',')\n",
    "    \n",
    "    return x_data, averaged_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the intensity correction data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_intensity_corr_data_cleaning = 'y'\n",
    "\n",
    "if do_intensity_corr_data_cleaning == 'y':\n",
    "    \n",
    "    do_median_filtering = 'n'\n",
    "    do_data_cleaning = 'n'\n",
    "    erp = 10 \n",
    "    k_size = 3\n",
    "    \n",
    "    print(\"\\nSelect tungsten halogen (Blue Ocean optics HL2000) lamp spectrum for intensity correction \\n(optional; if none is chosen, default intensity correction factor will be used): \")\n",
    "    selectHalLamp_1 = SelectFilesButton()\n",
    "    display(selectHalLamp_1)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:arial; font-size:16pt;\">Set the value of ***max_val_intensity*** to a number. The max of the intensity correction data should be above this value to be considered in further caluclation.\n",
    "<br>\n",
    "<br>\n",
    "The considered intensity correction file names will be printed and saved with ***_cleaned*** at the end of the original name with *.txt* extension.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_median_filtering = 'n'\n",
    "do_savgol_filtering = 'n'\n",
    "do_data_cleaning = 'n'\n",
    "erp = 10 \n",
    "k_size = 3\n",
    "k_size_savgol = 51\n",
    "p_order_savgol = 1\n",
    "    \n",
    "if do_intensity_corr_data_cleaning == 'y':\n",
    "\n",
    "    flamp_1=selectHalLamp_1.files\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(len(flamp_1))):\n",
    "        \n",
    "        head, tail = os.path.split(flamp_1[i])\n",
    "\n",
    "        x_data,y_data = get_data(flamp_1,i)\n",
    "        \n",
    "        if do_savgol_filtering == 'y':\n",
    "            savgol_filt_data = filter_savgol(y_data,k_size_savgol,p_order_savgol)\n",
    "        else:\n",
    "            savgol_filt_data = y_data\n",
    "\n",
    "        if do_median_filtering == 'y':\n",
    "            filt_data = filter_median(savgol_filt_data,k_size)\n",
    "        else:\n",
    "            filt_data = savgol_filt_data\n",
    "\n",
    "        if do_data_cleaning == 'y':\n",
    "            cleaned_data = data_cleaning(filt_data,erp)\n",
    "        else:\n",
    "            cleaned_data = filt_data\n",
    "            \n",
    "        max_val_intensity = 3000 ## set the value of intesnity that an intensity correction file should have to get considered\n",
    "\n",
    "        if np.max(cleaned_data)>max_val_intensity:\n",
    "            #print(flamp_1[i])\n",
    "\n",
    "            datamatrix = np.column_stack((x_data,cleaned_data)) # Compiles corrected data into a new matrix\n",
    "            savename = flamp_1[i][:-4]+\"_cleaned.txt\" # Create filename for new data\n",
    "            np.savetxt(savename,datamatrix) # Save new data\n",
    "            \n",
    "            #plt.plot(x_data,y_data)\n",
    "            plt.plot(x_data,cleaned_data,label=tail)\n",
    "            #plt.plot(x_data,filt_data)\n",
    "            #plt.plot(x_data,savgol_filt_data)\n",
    "            \n",
    "            plt.xlabel('Wavelength (nm)')\n",
    "            plt.ylabel('Intensity (a.u.)')\n",
    "            #plt.ylim(0,1000)\n",
    "            plt.legend()\n",
    "            #plt.ylim(100,250)\n",
    "            plt.title(tail)\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform intensity correction on the spectra data files\n",
    "\n",
    "<span style=\"font-family:arial; font-size:12pt;\"> Use the ***_cleaned*** intensity correction file from the above and upload it in the HalLamp below.\n",
    "    <br>\n",
    "    <br>\n",
    "    Upload the all spectra files in the first Select Files button.\n",
    "    The intensity corrected spectra data files will be stored in the same folder as the original file with a ***_calib*** at the end of the name. You can use this intensity calibrated file for future use and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Select data files you want intensity corrected: \")\n",
    "selectFiles = SelectFilesButton()\n",
    "display(selectFiles)\n",
    "\n",
    "do_intensity_correction = 'y'\n",
    "\n",
    "if do_intensity_correction == 'y':\n",
    "    print(\"Select data dark BKG: (Optional, don't use if background already subtracted)\")\n",
    "    selectDark = SelectFilesButton()\n",
    "    display(selectDark)\n",
    "    print(\"________________________________________________________________________________________________________\")\n",
    "\n",
    "    print(\"\\nSelect tungsten halogen (Blue Ocean optics HL2000) lamp spectrum for intensity correction \\n(optional; if none is chosen, default intensity correction factor will be used): \")\n",
    "    selectHalLamp = SelectFilesButton()\n",
    "    display(selectHalLamp)\n",
    "\n",
    "    print(\"\\nSelect baseline spectrum for tungsten lamp measurement: (Optional, don't use if background already subtracted)\")\n",
    "    selectBaseline = SelectFilesButton()\n",
    "    display(selectBaseline)\n",
    "    print(\"________________________________________________________________________________________________________\")\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables\n",
    "\n",
    "h = 6.626e-34 #Planck's constant in Js\n",
    "heV = 4.136e-15 #Planck's constant in eVs\n",
    "c = 2.9979e8 #Speed of light in m/s\n",
    "kb = 1.381e-23 #Boltzmann constant in J/K\n",
    "kbeV = 8.617e-5 #Boltzmann constant in eV/K\n",
    "#pixelArea = (20e-6)*(20e-6)*100; # LN Spectrometer Area of 100, 20um x 20um pixels per wavelength\n",
    "pixelArea = (14e-6)*(200e-6); # Ocean Optics UV Vis 14um x 200um per wavelength\n",
    "# acquisitionTime = 0.5; # Acquisition time is seconds\n",
    "acquisitionTime = 0.001 # 500ms acquistion time\n",
    "#T0 = 1000 # Initial temperature guess in degrees K\n",
    "\n",
    "do_intensity_correction = 'y'\n",
    "do_baseline_subtraction = 'n'\n",
    "do_median_filtering = 'n'\n",
    "do_data_cleaning = 'n'\n",
    "p_order = 5 # Polynomial order for baseline subtraction\n",
    "k_size = 3 # Kernel size for Median Filtering\n",
    "erp = -11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(r'H:\\My Data\\LH_data\\05102023**\\*.txt',recursive = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(columns=['file_name', 'Temperature(K)'])\n",
    "counter = 0\n",
    "\n",
    "fdata=selectFiles.files\n",
    "#fdark=selectDark.files\n",
    "flamp=selectHalLamp.files\n",
    "#flampbase=selectBaseline.files\n",
    "\n",
    "for file in tqdm(fdata): \n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('Integration Time (sec):'):\n",
    "                integration_time = line.split(':')[1].strip()\n",
    "                acquisitionTime = float(integration_time)\n",
    "                # print(acquisitionTime)\n",
    "    \n",
    "    head, tail = os.path.split(file)\n",
    "        \n",
    "    #print(head)\n",
    "    #print(tail)\n",
    "    #print(file)\n",
    "    \n",
    "    counter = counter + 1\n",
    "\n",
    "    x_data,y_data = get_data_2(file)\n",
    "    # print(x_data,y_data)\n",
    "    \n",
    "    if do_intensity_correction == 'y':\n",
    "        x_data,y_data,intens_data = i_corr(file,flamp)\n",
    "    else:\n",
    "        intens_data = y_data\n",
    "    \n",
    "    #norm_intens_data = intens_data/np.max(intens_data)\n",
    "    norm_intens_data=intens_data\n",
    "        \n",
    "    if do_median_filtering == 'y':\n",
    "        filt_data = filter_median(norm_intens_data,k_size)\n",
    "    else:\n",
    "        filt_data = norm_intens_data\n",
    "\n",
    "    if do_baseline_subtraction == 'y':\n",
    "        base = subtract_bsl(filt_data,p_order)\n",
    "    else:\n",
    "        base = 0\n",
    "        \n",
    "    bsl_subt_data = filt_data-base\n",
    "    \n",
    "    if do_data_cleaning == 'y':\n",
    "        cleaned_data = data_cleaning(bsl_subt_data,erp)\n",
    "    else:\n",
    "        cleaned_data = bsl_subt_data\n",
    "        \n",
    "    # Plancks fitting process\n",
    "\n",
    "    pixelResolution = (x_data[-1] - x_data[-2])  #delta wavelength nm\n",
    "    #print('Pixel Resolution = ' + str(pixelResolution))\n",
    "    \n",
    "    y_spec_irr = np.zeros(shape=len(x_data))\n",
    "    photonFlux = np.zeros(shape=len(cleaned_data))\n",
    "    for j in range(len(x_data)):\n",
    "        photonFlux[j] = cleaned_data[j]/(pixelArea*acquisitionTime)\n",
    "        y_spec_irr[j] = photonFlux[j]*(h*c/(x_data[j]*10**-9))/(pixelResolution) # spectral irradiance in W/m^2-nm\n",
    "    \n",
    "    # y_spec_irr_norm = y_spec_irr/np.max(y_spec_irr)\n",
    "    # y_spec_irr_norm = (y_spec_irr - np.min(y_spec_irr))/(np.max(y_spec_irr)-np.min(y_spec_irr))\n",
    "    y_spec_irr_norm = y_spec_irr\n",
    "\n",
    "    \n",
    "    #print(x)\n",
    "    start = np.argmin(abs(x_data[1:]-420))\n",
    "    end = np.argmin(abs(x_data[1:]-550))\n",
    "    #print(start, end)\n",
    "    wa = x_data[start:end]\n",
    "    ydata_1 = y_spec_irr_norm[start:end]\n",
    "    ydata_1 = ydata_1\n",
    "    print(max(ydata_1))\n",
    "\n",
    "    T0 = 820 # in K\n",
    "    \n",
    "    popt, pcov = curve_fit(blackbody_lam_2, wa, ydata_1, p0=T0)\n",
    "        \n",
    "    # get the best fitting parameter values and their 1 sigma errors\n",
    "    # (assuming the parameters aren't strongly correlated).\n",
    "\n",
    "    bestT1 = popt\n",
    "    sigmaT1  = np.sqrt(np.diag(pcov))\n",
    "\n",
    "    ybest = blackbody_lam_2(wa, bestT1)\n",
    "    yblack = blackbody_lam_2(wa, T0)\n",
    "\n",
    "    # print('True model values')\n",
    "    # print('  T0 = %.2f K' % T0)\n",
    "\n",
    "    # print('Parameters of best-fitting model:')\n",
    "    # print('  T1 = %.2f K +/- %.2f K' % (bestT1, sigmaT1))\n",
    "\n",
    "    # df_1.loc[i] = [file] + list[str(bestT1)]\n",
    "    \n",
    "    df_1.loc[counter] = [tail,bestT1[0]]\n",
    "\n",
    "    # degrees_of_freedom = len(wa) - 2\n",
    "    # resid = (ydata_1 - ybest) / sigmaT1\n",
    "    # chisq = np.dot(resid, resid)\n",
    "\n",
    "    # #print(degrees_of_freedom, 'dof')\n",
    "    # print('chi squared %.2f' % chisq) \n",
    "    # #print('nchi2 %.2f' % (chisq / degrees_of_freedom))\n",
    "    \n",
    "    # plot the solution\n",
    "    plt.plot(wa,ydata_1,label='Intensity corrected data')\n",
    "    #plt.xlim(360,850)\n",
    "    #plt.ylim(0,15)\n",
    "    plt.plot(wa,yblack,label='Input Temperature %d'%T0)\n",
    "    plt.plot(wa, ybest, label='Best fitting model %d'%bestT1)\n",
    "    plt.title(\"Planck's emission from G4-doped CA in MEMC BDAC (Ar PTM) - 1070 nm LH\")\n",
    "    plt.ylabel(\"Spectral Irradiance (W/m$^2$-nm)\")\n",
    "    plt.xlabel(\"Wavelength (nm)\")\n",
    "    plt.legend(frameon=True)\n",
    "    #plt.savefig('')\n",
    "    plt.show()\n",
    "print(df_1)\n",
    "# df_1.to_csv(r'temp_val_2.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1 = pd.read_csv('temp_val_2.csv')\n",
    "\n",
    "x = df_1['file_name'].to_numpy()\n",
    "y = df_1['Temperature(K)'].to_numpy()\n",
    "\n",
    "#fig1 = plt.figure()\n",
    "#plt.axis([0, 10, 0, 1])\n",
    "\n",
    "#for i in range(316):\n",
    "    #y = np.random.random()\n",
    "for idx, yi in enumerate(y):\n",
    "    if yi>1200:\n",
    "        plt.scatter(idx,yi)\n",
    "plt.xlabel('time (a.u.)')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.title('Best Fit Temperature for one spot - Planck emission')\n",
    "    #plt.pause(0.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata=selectFiles.files\n",
    "fdark=selectDark.files\n",
    "flamp=selectHalLamp.files\n",
    "flampbase=selectBaseline.files\n",
    "\n",
    "for i in range(0,len(fdata)):\n",
    "    \n",
    "    x_data,y_data = get_data(fdata,i)\n",
    "    \n",
    "    #print(len(y_data))\n",
    "    \n",
    "    if do_intensity_correction == 'y':\n",
    "        x_data,y_data,intens_data = i_corr(fdata,i)\n",
    "    else:\n",
    "        intens_data = y_data\n",
    "    \n",
    "    #norm_intens_data = intens_data/np.max(intens_data)\n",
    "    norm_intens_data=intens_data\n",
    "    #plt.figure()\n",
    "    #plt.plot(x_data,norm_intens_data,label='Normalized Data')\n",
    "    #plt.plot(x_data,y_data/np.max(y_data),label='Actual Data')\n",
    "    #plt.plot(x_data,intens_data/np.max(intens_data),label='Intensity Corrected data')\n",
    "    #plt.title(fdata[i])\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "        \n",
    "    if do_median_filtering == 'y':\n",
    "        filt_data = filter_median(norm_intens_data,k_size)\n",
    "    else:\n",
    "        filt_data = norm_intens_data\n",
    "        \n",
    " \n",
    "    if do_baseline_subtraction == 'y':\n",
    "        base = subtract_bsl(filt_data,p_order)\n",
    "    else:\n",
    "        base = 0\n",
    "        \n",
    "    bsl_subt_data = filt_data-base\n",
    "    \n",
    "    if do_data_cleaning == 'y':\n",
    "        cleaned_data = data_cleaning(bsl_subt_data,erp)\n",
    "    else:\n",
    "        cleaned_data = bsl_subt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvl_lam = np.linspace(10,2000,10000)\n",
    "temp_val = np.linspace(800,3000,22)\n",
    "\n",
    "for i in range(len(temp_val)):\n",
    "    bl_rad = blackbody_lam(wvl_lam,temp_val[i])\n",
    "    \n",
    "    plt.plot(wvl_lam,bl_rad,label='%d'%temp_val[i])\n",
    "    plt.legend()\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "for j in range(len(temp_val)):\n",
    "    bl_rad =  (wvl_lam,temp_val[j])\n",
    "    \n",
    "    plt.plot(wvl_lam,bl_rad,label='%d'%temp_val[j])\n",
    "    plt.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "bdbce315960c3776fe868f285029d7a59084f2e8882315e6dd1aa58e67f790cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
